{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13T17:13:01.000Z</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374</td>\n",
       "      <td>57527</td>\n",
       "      <td>2966</td>\n",
       "      <td>15954</td>\n",
       "      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ZAPwfrtAFY</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T07:30:00.000Z</td>\n",
       "      <td>last week tonight trump presidency|\"last week ...</td>\n",
       "      <td>2418783</td>\n",
       "      <td>97185</td>\n",
       "      <td>6146</td>\n",
       "      <td>12703</td>\n",
       "      <td>https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>One year after the presidential election, John...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12T19:05:24.000Z</td>\n",
       "      <td>racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...</td>\n",
       "      <td>3191434</td>\n",
       "      <td>146033</td>\n",
       "      <td>5339</td>\n",
       "      <td>8181</td>\n",
       "      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH MY PREVIOUS VIDEO ‚ñ∂ \\n\\nSUBSCRIBE ‚ñ∫ http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>puqaWrEC7tY</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Nickelback Lyrics: Real or Fake?</td>\n",
       "      <td>Good Mythical Morning</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T11:00:04.000Z</td>\n",
       "      <td>rhett and link|\"gmm\"|\"good mythical morning\"|\"...</td>\n",
       "      <td>343168</td>\n",
       "      <td>10172</td>\n",
       "      <td>666</td>\n",
       "      <td>2146</td>\n",
       "      <td>https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Today we find out if Link is a Nickelback amat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d380meD0W0M</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>I Dare You: GOING BALD!?</td>\n",
       "      <td>nigahiga</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-12T18:01:41.000Z</td>\n",
       "      <td>ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...</td>\n",
       "      <td>2095731</td>\n",
       "      <td>132235</td>\n",
       "      <td>1989</td>\n",
       "      <td>17518</td>\n",
       "      <td>https://i.ytimg.com/vi/d380meD0W0M/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>I know it's been a while since we did this sho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  2kyS6SvSYSE      17.14.11   \n",
       "1  1ZAPwfrtAFY      17.14.11   \n",
       "2  5qpjK5DgCt4      17.14.11   \n",
       "3  puqaWrEC7tY      17.14.11   \n",
       "4  d380meD0W0M      17.14.11   \n",
       "\n",
       "                                               title          channel_title  \\\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE           CaseyNeistat   \n",
       "1  The Trump Presidency: Last Week Tonight with J...        LastWeekTonight   \n",
       "2  Racist Superman | Rudy Mancuso, King Bach & Le...           Rudy Mancuso   \n",
       "3                   Nickelback Lyrics: Real or Fake?  Good Mythical Morning   \n",
       "4                           I Dare You: GOING BALD!?               nigahiga   \n",
       "\n",
       "   category_id              publish_time  \\\n",
       "0           22  2017-11-13T17:13:01.000Z   \n",
       "1           24  2017-11-13T07:30:00.000Z   \n",
       "2           23  2017-11-12T19:05:24.000Z   \n",
       "3           24  2017-11-13T11:00:04.000Z   \n",
       "4           24  2017-11-12T18:01:41.000Z   \n",
       "\n",
       "                                                tags    views   likes  \\\n",
       "0                                    SHANtell martin   748374   57527   \n",
       "1  last week tonight trump presidency|\"last week ...  2418783   97185   \n",
       "2  racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...  3191434  146033   \n",
       "3  rhett and link|\"gmm\"|\"good mythical morning\"|\"...   343168   10172   \n",
       "4  ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...  2095731  132235   \n",
       "\n",
       "   dislikes  comment_count                                  thumbnail_link  \\\n",
       "0      2966          15954  https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg   \n",
       "1      6146          12703  https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg   \n",
       "2      5339           8181  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n",
       "3       666           2146  https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg   \n",
       "4      1989          17518  https://i.ytimg.com/vi/d380meD0W0M/default.jpg   \n",
       "\n",
       "   comments_disabled  ratings_disabled  video_error_or_removed  \\\n",
       "0              False             False                   False   \n",
       "1              False             False                   False   \n",
       "2              False             False                   False   \n",
       "3              False             False                   False   \n",
       "4              False             False                   False   \n",
       "\n",
       "                                         description  \n",
       "0  SHANTELL'S CHANNEL - https://www.youtube.com/s...  \n",
       "1  One year after the presidential election, John...  \n",
       "2  WATCH MY PREVIOUS VIDEO ‚ñ∂ \\n\\nSUBSCRIBE ‚ñ∫ http...  \n",
       "3  Today we find out if Link is a Nickelback amat...  \n",
       "4  I know it's been a while since we did this sho...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dataset/USvideos.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out videos that have ratings disabled or have no likes\n",
    "df = df[df[\"ratings_disabled\"] == False]\n",
    "df = df[df[\"likes\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"dataset/US_category_id.json\") as f:\n",
    "    data = json.load(f)\n",
    "    categories = {int(item[\"id\"]): item[\"snippet\"][\"title\"] for item in data[\"items\"]}\n",
    "\n",
    "\n",
    "def remove_punc_and_lower(words: str):\n",
    "    cleaned = \"\".join(c for c in words.lower() if c not in string.punctuation)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "df[\"category\"] = df[\"category_id\"].map(categories)\n",
    "\n",
    "df[\"title\"] = df[\"title\"].apply(remove_punc_and_lower)\n",
    "\n",
    "df[\"tags\"] = df[\"tags\"].apply(lambda x: \" \".join(x.split(\"|\")))\n",
    "df[\"tags\"] = df[\"tags\"].apply(lambda x: \"\".join(x.split('\"')))\n",
    "df[\"tags\"] = df[\"tags\"].apply(remove_punc_and_lower)\n",
    "\n",
    "df[\"description\"] = df[\"description\"].fillna(\"\")\n",
    "df[\"description\"] = df[\"description\"].apply(remove_punc_and_lower)\n",
    "\n",
    "df[\"publish_time\"] = pd.to_datetime(df[\"publish_time\"]).dt.tz_convert(\"US/Pacific\")\n",
    "\n",
    "df[\"day_of_week\"] = df[\"publish_time\"].dt.day_name()\n",
    "\n",
    "df[\"dislikes\"] = df.apply(\n",
    "    lambda row: row[\"dislikes\"] if row[\"dislikes\"] > 0 else 1, axis=1\n",
    ")\n",
    "df[\"likes_dislikes_ratio\"] = df[\"likes\"] / df[\"dislikes\"]\n",
    "\n",
    "feature_cols = [\"title\", \"tags\", \"description\", \"category\", \"day_of_week\"]\n",
    "X = df[feature_cols]\n",
    "y = df[\"likes_dislikes_ratio\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5180</th>\n",
       "      <td>diy pizza pouch  wearable pizza</td>\n",
       "      <td>pizza pouch the pizza pouch the pizza bag pizz...</td>\n",
       "      <td>pizza pouches might be the most practical thin...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19960</th>\n",
       "      <td>trace cyrus brenda official lyric video</td>\n",
       "      <td>trace cyrus brenda song metro station shake it...</td>\n",
       "      <td>happy valentines day üíîüíïüñ§üíò‚ù§Ô∏è</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18736</th>\n",
       "      <td>why i left nickelodeon</td>\n",
       "      <td>nickelodeon nick butch hartman why i left buzz...</td>\n",
       "      <td>after 20 years of creating shows like fairly o...</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>sia  ho ho ho</td>\n",
       "      <td>sia ho ho ho holiday</td>\n",
       "      <td>‚Äúho ho ho from everyday is christmas  out ever...</td>\n",
       "      <td>Music</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>ranz and niana goes to la carpool around</td>\n",
       "      <td>ranz ranz kyle niana niana guerrero ranz niana...</td>\n",
       "      <td>so we were invited to watch the premiere and a...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "5180            diy pizza pouch  wearable pizza   \n",
       "19960   trace cyrus brenda official lyric video   \n",
       "18736                    why i left nickelodeon   \n",
       "1516                              sia  ho ho ho   \n",
       "1119   ranz and niana goes to la carpool around   \n",
       "\n",
       "                                                    tags  \\\n",
       "5180   pizza pouch the pizza pouch the pizza bag pizz...   \n",
       "19960  trace cyrus brenda song metro station shake it...   \n",
       "18736  nickelodeon nick butch hartman why i left buzz...   \n",
       "1516                                sia ho ho ho holiday   \n",
       "1119   ranz ranz kyle niana niana guerrero ranz niana...   \n",
       "\n",
       "                                             description          category  \\\n",
       "5180   pizza pouches might be the most practical thin...     Entertainment   \n",
       "19960                        happy valentines day üíîüíïüñ§üíò‚ù§Ô∏è    People & Blogs   \n",
       "18736  after 20 years of creating shows like fairly o...  Film & Animation   \n",
       "1516   ‚Äúho ho ho from everyday is christmas  out ever...             Music   \n",
       "1119   so we were invited to watch the premiere and a...    People & Blogs   \n",
       "\n",
       "      day_of_week  \n",
       "5180       Sunday  \n",
       "19960   Wednesday  \n",
       "18736    Thursday  \n",
       "1516     Thursday  \n",
       "1119     Thursday  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_title = TfidfVectorizer(\n",
    "    max_features=500, stop_words=\"english\", ngram_range=(1, 3)\n",
    ")\n",
    "tfidf_tags = TfidfVectorizer(max_features=300, stop_words=\"english\", ngram_range=(1, 3))\n",
    "tfidf_description = TfidfVectorizer(\n",
    "    max_features=700, stop_words=\"english\", ngram_range=(1, 3)\n",
    ")\n",
    "\n",
    "X_train_title = tfidf_title.fit_transform(X_train[\"title\"]).toarray()\n",
    "X_train_tags = tfidf_tags.fit_transform(X_train[\"tags\"]).toarray()\n",
    "X_train_description = tfidf_description.fit_transform(X_train[\"description\"]).toarray()\n",
    "\n",
    "X_test_title = tfidf_title.transform(X_test[\"title\"]).toarray()\n",
    "X_test_tags = tfidf_tags.transform(X_test[\"tags\"]).toarray()\n",
    "X_test_description = tfidf_description.transform(X_test[\"description\"]).toarray()\n",
    "\n",
    "# Categorical Features\n",
    "category_encoder = OneHotEncoder(sparse_output=False)\n",
    "category_train = category_encoder.fit_transform(X_train[[\"category\"]])\n",
    "category_test = category_encoder.transform(X_test[[\"category\"]])\n",
    "\n",
    "# Temporal Features\n",
    "day_encoder = OneHotEncoder(sparse_output=False)\n",
    "day_of_week_train = day_encoder.fit_transform(X_train[[\"day_of_week\"]])\n",
    "day_of_week_test = day_encoder.transform(X_test[[\"day_of_week\"]])\n",
    "\n",
    "\n",
    "X_train_combined = np.hstack(\n",
    "    [\n",
    "        X_train_title,\n",
    "        X_train_tags,\n",
    "        X_train_description,\n",
    "        category_train,\n",
    "        day_of_week_train,\n",
    "    ]\n",
    ")\n",
    "X_test_combined = np.hstack(\n",
    "    [\n",
    "        X_test_title,\n",
    "        X_test_tags,\n",
    "        X_test_description,\n",
    "        category_test,\n",
    "        day_of_week_test,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/aaronang/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "\n",
    "def text_to_avg_w2v(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return get_average_word2vec(tokens, word2vec_model)\n",
    "\n",
    "\n",
    "def get_average_word2vec(tokens, model, vector_size=300):\n",
    "    vectors = [model[token] for token in tokens if token in model]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "\n",
    "X_train_title_w2v = np.array([text_to_avg_w2v(text) for text in X_train[\"title\"]])\n",
    "X_train_tags_w2v = np.array([text_to_avg_w2v(text) for text in X_train[\"tags\"]])\n",
    "X_train_description_w2v = np.array(\n",
    "    [text_to_avg_w2v(text) for text in X_train[\"description\"]]\n",
    ")\n",
    "X_train_w2v = np.hstack(\n",
    "    [\n",
    "        X_train_title_w2v,\n",
    "        X_train_tags_w2v,\n",
    "        X_train_description_w2v,\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_test_title_w2v = np.array([text_to_avg_w2v(text) for text in X_test[\"title\"]])\n",
    "X_test_tags_w2v = np.array([text_to_avg_w2v(text) for text in X_test[\"tags\"]])\n",
    "X_test_description_w2v = np.array(\n",
    "    [text_to_avg_w2v(text) for text in X_test[\"description\"]]\n",
    ")\n",
    "X_test_w2v = np.hstack(\n",
    "    [\n",
    "        X_test_title_w2v,\n",
    "        X_test_tags_w2v,\n",
    "        X_test_description_w2v,\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_combined = np.hstack(\n",
    "    [\n",
    "        X_train_combined,\n",
    "        X_train_w2v,\n",
    "    ]\n",
    ")\n",
    "X_test_combined = np.hstack(\n",
    "    [\n",
    "        X_test_combined,\n",
    "        X_test_w2v,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_combined = scaler.fit_transform(X_train_combined)\n",
    "X_test_combined = scaler.transform(X_test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1438.1497, MAE: 22.9766, R2: 0.5401\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_combined, y_train)\n",
    "predictions = lr.predict(X_test_combined)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 672.9047, MAE: 14.0239, R2: 0.7848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "gbr = HistGradientBoostingRegressor(random_state=42)\n",
    "gbr.fit(X_train_combined, y_train)\n",
    "predictions = gbr.predict(X_test_combined)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 353.2444, MAE: 10.4837, R2: 0.8870\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "bst = XGBRegressor(random_state=42)\n",
    "bst.fit(X_train_combined, y_train)\n",
    "predictions = bst.predict(X_test_combined)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 227.2140, MAE: 5.2698, R2: 0.9273\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "rfr.fit(X_train_combined, y_train)\n",
    "predictions = rfr.predict(X_test_combined)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
